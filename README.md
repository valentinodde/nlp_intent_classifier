
# Evaluating the Robustness of NLP Classification Models to Typographic Error

This repository contains the code for the paper "Evaluating the Robustness of NLP Classification Models to Typographic Error". The paper investigates the impact of typographic error on the prediction of NLP classification models and proposes a method to evaluate the robustness of a given model to typographic error.

## Authors

- **Valentin Odde** - *AI master student* - [valentinodde](https://github.com/valentinodde)
- **Matthieu Annequin** - *AI master student* - [matthieuannequin](https://github.com/matthieuannequin)


## About the project

Project realized in the framework of the NLP course of CentralSup√©lec, supervised by Pierre Colombo.


## Requirements

To run the code, you will need:
- Python 3.9 or higher

The script use some huggingface library, but installation are made on the notebook.

## Getting Started

1. Clone the repository:

```sh
git clone https://github.com/valentinodde/nlp_intent_classifier
```
2. Open the notebook **`Intentclassification.ipynb`** or **`NLP_project.ipynb`**

3. We advise you to use an environment with a GPU in order to train the model in a reasonable time. For instance you can use **Google Collab** free instances.
